{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 4. Конструирование текстовых признаков из твитов пользователей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сбор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала импортируем все чем будем пользоваться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sys import stdout\n",
    "from time import sleep\n",
    "from collections import defaultdict\n",
    "import codecs\n",
    "import pickle\n",
    "\n",
    "import twitter\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from pytagcloud import create_tag_image, make_tags\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый этап - сбор твитов пользователей. Необходимо подключаться к Twitter API и запрашивать твиты по id пользователя. \n",
    "Подключение к API подробно описано в ДЗ 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "CONSUMER_KEY = \"Il7TATLVOo64Fvdzz1XPBscBD\"\n",
    "CONSUMER_SECRET = \"EaZBgG2QAGjbb3nASxzbXUwBhkmj0P1geXP8EtiuNxAm484rsD\"\n",
    "\n",
    "ACCESS_TOKEN_KEY = \"3064704131-3zaVpY40gSd1kAP163yoMAlP2tqcBLG11qYjIh8\"\n",
    "ACCESS_TOKEN_SECRET = \"cnIzGKo5NQPwTVQegIqT5Lv7IeeLCvznNn3lgouzIEOrK\"\n",
    "\n",
    "api = twitter.Api(consumer_key=CONSUMER_KEY, \n",
    "                  consumer_secret=CONSUMER_SECRET, \n",
    "                  access_token_key=ACCESS_TOKEN_KEY, \n",
    "                  access_token_secret=ACCESS_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения твитов пользователя может быть использован метод GetUserTimeline из библиотеки python-twitter. Он позволяет получить не более 200 твитов пользователя.\n",
    "\n",
    "Метод имеет ограничение по количеству запросов в секунду. Для получения информации о промежутке времени, которое необходимо подождать для повторного обращения к API может быть использован метод `GetSleepTime`. Для получения информации об ограничениях запросов с помощью метода `GetUserTimeLine` необходимо вызывать `GetSleepTime` с параметром \"statuses/user_timeline\".\n",
    "\n",
    "Метод GetUserTimeline возвращает объекты типа Status. У этих объектов есть метод AsDict, который позволяет представить твит в виде словаря.\n",
    "\n",
    "Id пользователей необходимо считать из файла, как было сделано в ДЗ 1.\n",
    "\n",
    "Необходимо реализовать функцию `get_user_tweets(user_id)`. Входной параметр - id пользователя из файла. Возвращаемое значение - массив твитов пользователя, где каждый твит представлен в виде словаря. Предполагается, что информация о пользователе содержится в твитах, которые пользователь написал сам. Это означает, что можно попробовать отфильтровать ответы другим пользователям, ссылки и ретвиты, а так же картинки и видео, так как наша цель - найти текстовую информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_user_tweets(user_id):\n",
    "    return [st.AsDict() for st in api.GetUserTimeline(user_id=user_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве теста получим последние твиты кого-то кто назвался Bred Pit :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'RT @sashasasha3506: http://t.co/VNhzKyiKA0', u'RT @AknKarc: \"Don\\'t act, be yourself !\"', u'WTF= Welcome To Facebook :)', u'gotta be kidding...umm', u'so far away...', u'soo good']\n"
     ]
    }
   ],
   "source": [
    "pit_tweets = get_user_tweets(144808131)\n",
    "pit_texts = [dt['text'] for dt in pit_tweets]\n",
    "\n",
    "print(pit_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбор текста твита"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка текста предполагает разбиение текста на отдельные элементы - параграфы, предложения, слова. Мы будем преобразовывать текст твита к словам. Для этого текст необходимо разбить на слова. Сделать это можно, например, с помощью регулярного выражения.\n",
    "\n",
    "Необходимо реализовать функцию, `get_words(text)`. Входной параметр - строка с текстом. Возвращаемое значение - массив строк (слов). Обратите внимание, что нужно учесть возможное наличие пунктуации и выделить по возможности только слова. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_words(text):\n",
    "    \"\"\"returns list of words\"\"\"\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here', 'are', 'different', 'words', '!']\n"
     ]
    }
   ],
   "source": [
    "print(get_words(\"Here are different words!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее полученные слова необходимо привести к нормальной форме. То есть привести их к форме единственного числа настоящего времени и пр. Сделать это можно с помощью библиотеки nltk. Информацию по загрузке, установке библиотеки и примерах использования можно найти на сайте http://www.nltk.org/\n",
    "\n",
    "Для загрузки всех необходимых словарей можно воспользоваться методом download из библиотеки nltk.\n",
    "\n",
    "Но так как мы все загрузили на занятии, смело уберем его."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для дальнейшей обработки слова должны быть приведены к нижнему регистру. \n",
    "\n",
    "Для приведения к нормальной форме можно использовать `WordNetLemmatizer` из библиотеки nltk. У этого класса есть метод `lemmatize`.\n",
    "\n",
    "Также необходимо убрать из текста так называемые стоп-слова. Это часто используемые слова, не несущие смысловой нагрузки для наших задач. Сделать это можно с помощью `stopwords` из nltk.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо реализовать функцию `get_tokens(words)`. Входной параметр - массив слов. Возвращаемое значение - массив токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tokens(words):\n",
    "    stop = nltk.corpus.stopwords.words('english')\n",
    "    wnl = WordNetLemmatizer()\n",
    "    words = [wnl.lemmatize(wrd.lower()) for wrd in words]\n",
    "    return[wrd for wrd in words if wrd not in stop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['different', u'word']\n"
     ]
    }
   ],
   "source": [
    "print(get_tokens([\"here\", \"are\", \"different\", \"words\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо реализовать функцию `get_tweet_tokens(tweet)`. Входной параметр - текст твита. Возвращаемое значение -- токены твита. \n",
    "\n",
    "Здесь наивным образом офильтруем фото, видео и ссылки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tweet_tokens(tweet):\n",
    "    if \"http:\" not in tweet and \"/\" not in tweet and \"@\" not in tweet:\n",
    "        return get_tokens(get_words(tweet))\n",
    "    else:\n",
    "        return list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо реализовать функцию `collect_users_tokens()`. Функция должна сконструировать матрицу признаков пользователей. В этой матрице строка - пользователь. Столбец - токен. На пересечении - сколько раз токен встречается у пользователя.\n",
    "Для построения матрицы можно использовать `DictVectorizer` из `sklearn.feature_extraction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collect_users_tokens(src_tweets):\n",
    "    \"\"\"returns users list and list of user dicts. Each dict contains frequence of user tokens\"\"\"\n",
    "    id_to_tokens = dict()\n",
    "    for dt in src_tweets:\n",
    "        if dt[\"user\"][\"id\"] not in id_to_tokens.keys():\n",
    "            id_to_tokens[dt[\"user\"][\"id\"]] = dict()\n",
    "        \n",
    "        text = dt[\"text\"]\n",
    "        tokens = get_tweet_tokens(text)\n",
    "        \n",
    "        for tk in tokens:\n",
    "            if tk not in id_to_tokens[dt[\"user\"][\"id\"]].keys():\n",
    "                id_to_tokens[dt[\"user\"][\"id\"]][tk] = 0\n",
    "            id_to_tokens[dt[\"user\"][\"id\"]][tk] += 1\n",
    "    \n",
    "    ids = list(id_to_tokens.keys())\n",
    "    \n",
    "    return ids, [id_to_tokens[i] for i in ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users 8000:\n",
      "Processed 0 users\n",
      "Sleeping 0\n",
      "Sleeping 0\n"
     ]
    }
   ],
   "source": [
    "TRAINING_SET_URL = \"../files/twitter_train.txt\"\n",
    "df_users = pd.read_csv(TRAINING_SET_URL, sep=\",\", header=0)\n",
    "\n",
    "src_tweets = list()\n",
    "\n",
    "print (\"Total users %s:\" % len(df_users[\"Id\"]))\n",
    "\n",
    "for el, id_ in enumerate(df_users[\"Id\"][:40]):\n",
    "    if el % 100 == 0:\n",
    "        print(\"Processed %s users\" % el)\n",
    "        stdout.flush()\n",
    "    try:\n",
    "        src_tweets += get_user_tweets(id_)\n",
    "    except:\n",
    "        tm = api.GetSleepTime(\"statuses/user_timeline\")\n",
    "        print(\"Sleeping %s\" % tm)\n",
    "        stdout.flush()\n",
    "        sleep(tm)\n",
    "\n",
    "with open(\"../files/src_tweets.dat\", \"w\") as f:\n",
    "    pickle.dump(src_tweets, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users, users_tokens = collect_users_tokens(src_tweets)\n",
    "v = DictVectorizer()\n",
    "vs = v.fit_transform(users_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте чуть-чуть посмотрим на получившиеся наборы слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[984121344, 601849857, 215056389, 248807431, 61931528, 2352595372, 102137866, 391413781, 2962522134, 1683980311] [{u'wid': 1, u'hell': 1, u'resurrected': 1, u'feel': 2, u'mom': 1, u'fuck': 1, u'dead': 1, u'one': 1, u'today': 2, u'done': 1, u'ur': 1, u'year': 1, u'watched': 1, u'still': 1, u'lolita': 2, u'technically': 1, u'hydrocodone': 1, u'thigh': 1, u'yoga': 1, u'stair': 1, u'floor': 1, u'wtfff': 1, u'movie': 1, u'torn': 1, u'actually': 1, u'book': 1, u'loll': 1, u'pissed': 1, u'sick': 1, u'2013': 1, u'gave': 1, u'gravestone': 1, u'head': 1, u'full': 1, u\"'s\": 1, u'boy': 1, u'read': 1, u'class': 1, u'burn': 1, u'\\U0001f47d\\U0001f60e': 1, u'broke': 1, u'fucked': 1, u\"'m\": 2, u'cara': 1, u'totally': 1, u'mcr': 1, u'died': 1, u'going': 1, u'like': 2, u'getting': 1, u'leather': 1, u'die': 1, u'favorite': 1, u'fux': 1, u\"'ve\": 1, u'jacket': 1, u'length': 1, u'lingerie': 1, u'nose': 1, u'aesthetic': 1, u'history': 1}, {}, {u'swaggy': 1, u'emotion': 1, u'right': 1, u'love': 3, u'illegally': 1, u'alive': 1, u'jelly': 1, u\"''\": 3, u\"n't\": 2, u'see': 1, u'peanut': 1, u'go': 1, u'need': 1, u'hell': 1, u'trusted': 1, u'\\u2764\\ufe0f\\U0001f499': 1, u'gon': 1, u'glass': 1, u'na': 1, u'ease': 1, u',': 5, u'carry': 1, u'.': 5, u'forehead': 1, u'-natty': 1, u'live': 2, u'\\U0001f339\\U0001f339\\U0001f339': 1, u'legally': 2, u'jayhawk': 1, u'greatest': 1, u'box': 1, u'``': 3, u'buy': 1, u'pain': 1, u'rude': 1, u'rain': 1, u'bound': 1, u'phone': 1, u'roommate': 2, u'let': 1, u'know': 1, u'lighter': 1, u\"'m\": 2, u'shoot': 1, u'bro': 1, u'case': 1, u'shitty': 1, u'ever': 1, u'could': 1, u'gun': 1, u'chalk': 1, u'person': 1, u\"'re\": 2, u'butter': 1, u'ku': 1, u'rock': 1}, {u'play': 1, u'stream': 1, u'live': 1, u'soon': 1, u'next': 1, u'game': 1, u'im': 1, u'.': 1, u'nv': 1, u'joining': 1}, {u'certain': 1, u'mind': 1, u'fellow': 1, u'see': 1, u'respect': 1, u'everyday': 1, u'shit': 1, u'-3-7': 1, u')': 1, u'(': 1, u'racism': 1, u'.': 2, u'enough': 1, u'lot': 1, u'got': 1, u'?': 1, u'\\U0001f629\\U0001f3b6': 1, u'good': 2, u\"'s\": 1, u'rain': 1, u'making': 1, u'man': 1, u'always': 1, u'morning': 2, u'compliment': 1, u'amount': 1, u'stand': 1, u'pick': 1, u'tall': 1, u'\\U0001f4b5\\U0001f4b5\\U0001f4b5\\U0001f519': 1, u'|': 2}, {u'comment': 1, u'everyone': 1, u'stated': 1, u'list': 1, u'one': 1, u'subscribe': 1, u'tag': 1, u'want': 4, u'ps3': 2, u'open': 1, u'sssniperwolf': 1, u'awsome': 1, u'bo2': 2, u'send': 1, u'add': 1, u'text': 1, u'friend': 1, u'cod': 2, u'play': 5, u\"'s\": 2, u'gear': 1, u'wa': 1, u'use': 1, u'plz': 1, u'game': 1, u'gta5': 3, u\"'m\": 2, u'come': 1, u'lobby': 1, u'ghost': 1, u'join': 3, u'getting': 1, u'always': 2, u'metal': 1, u'psn': 1, u'disc': 1, u'u': 3, u'gamer': 1, u'guy': 2, u'playing': 2}, {}, {u'breakdown': 1, u'#': 1, u'everyone': 1, u\"'s\": 2, u'success': 1, u'process': 1, u'create': 1, u'communicate': 1, u',': 2, u'.': 3, u'person': 1, u'assume': 1, u'want': 1, u'organization': 1}, {u'feed': 1, u'fold': 1, u'\\U0001f62b\\U0001f62b': 1, u'go': 1, u'gm': 1, u'bingo': 1, u'wan': 1, u'would': 1, u'na': 1, u'two': 1, u'everything': 1, u'need': 1, u'jestin': 1, u'way': 1, u'tore': 1, u'starving': 1, u\"'s\": 1, u'tyj': 1, u'food': 1, u'\\U0001f62b\\U0001f62b\\U0001f62b': 1, u'\\U0001f601\\U0001f601': 1, u'school': 1, u'\\U0001f62b\\U0001f62b\\U0001f602\\U0001f602': 1, u'boring': 1, u'outing': 1, u\"n't\": 1, u'bag': 1, u'basket': 1, u'\\U0001f64c\\U0001f3fd\\U0001f64c\\U0001f3fd': 1, u'think': 1, u'clothes': 1}, {u'beautiful': 1, u'crazy': 1, u'love': 2, u'nursing': 1, u'feel': 1, u'beginning': 1, u'penny': 2, u'$': 1, u'replace': 1, u'shit': 1, u'\\U0001f611': 1, u'bitch': 1, u'actually': 1, u'world': 1, u'touch': 1, u'close': 1, u'\\U0001f64c\\U0001f64f': 1, u'intellect': 1, u'truly': 1, u'!': 3, u'wanted': 1, u'bih': 1, u'anymore': 1, u'&': 1, u'pay': 2, u'make': 1, u'remember': 1, u',': 1, u'.': 8, u'1': 1, u'owe': 1, u'n': 1, u'come': 1, u'best': 1, u'way': 3, u'money': 1, u'new': 1, u';': 1, u'told': 1, u'...': 1, u'life': 2, u'woman': 1, u\"'s\": 2, u'wa': 2, u'far': 1, u'offered': 1, u'blessing': 2, u'else': 1, u\"'m\": 1, u'job': 1, u'moving': 1, u'mad': 1, u'mom': 1, u'making': 1, u'nothing': 1, u'amp': 1, u'\\U0001f4af': 1, u'one': 1, u'opportunity': 1, u'candy': 1, u'man': 1, u'every': 1, u'great': 1, u'applying': 1, u'like': 2, u'\\u2764': 1, u'playin': 1, u'\\U0001f49c': 1, u'could': 3, u'take': 1, u'betta': 1, u'together': 1, u'along': 1, u\"n't\": 2, u'ain': 1, u'u': 1, u'\\U0001f602': 1, u'position': 1}]\n"
     ]
    }
   ],
   "source": [
    "print(users[:10], users_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним полученные данные в файл. Используется метод savez из numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savez(\"../files/out_4.dat\", data=vs, users=users, users_tokens=users_tokens )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее для получения представления о полученной информацию о токенах предлагается отобразить ее в виде облака тэгов. [Подсказка](http://anokhin.github.io/img/tag_cloud.png). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_tag_cloud(v, vs):\n",
    "    \"\"\"Draws tag cloud of found tokens\"\"\"\n",
    "    res = dict()\n",
    "    for dt in vs:\n",
    "        for el in dt:\n",
    "            try:\n",
    "                el1 = el.encode('ascii')\n",
    "                if el1 not in res.keys():\n",
    "                    res[el1] = 0\n",
    "                res[el1] += 1\n",
    "            except:\n",
    "                pass\n",
    "    tags = make_tags([(k, res[k]) for k in res])\n",
    "    create_tag_image(tags[:100], '/Users/feldsherov/Desktop/cloud_large.png', fontname='Molengo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "draw_tag_cloud(v, users_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
